{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial steps\n",
    "\n",
    "You require the following libraries for this lab.\n",
    "\n",
    "pandas library for data storage and manipulation.\n",
    "\n",
    "BeautifulSoup library for interpreting the HTML document.\n",
    "\n",
    "requests library to communicate with the web page.\n",
    "\n",
    "sqlite3 for creating the database instance.\n",
    "\n",
    "While requests and sqlite3 come bundled with Python3, you need to install pandas and BeautifulSoup libraries to the IDE.\n",
    "\n",
    "For this, run the following commands in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->bs4) (2.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of known entities\n",
    "\n",
    "You must declare a few entities at the beginning. For example, you know the required URL, the CSV name for saving the record, the database name, and the table name for storing the record. You also know the entities to be saved. Additionally, since you require only the top 50 results, you will require a loop counter initialized to 0. You may initialize all these by using the following code in webscraping_movies.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://web.archive.org/web/20230902185655/https://en.everybodywiki.com/100_Most_Highly-Ranked_Films'\n",
    "db_name = 'Movies.db'\n",
    "table_name = 'Top_50'\n",
    "csv_path = './top_50_films.csv'\n",
    "df = pd.DataFrame(columns=[\"Average Rank\",\"Film\",\"Year\"])\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the webpage for Webscraping\n",
    "\n",
    "To access the required information from the web page, you first need to load the entire web page as an HTML document in python using the requests.get().text function and then parse the text in the HTML format using BeautifulSoup to enable extraction of relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page = requests.get(url).text\n",
    "data = BeautifulSoup(html_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping of required information\n",
    "\n",
    "You now need to write the loop to extract the appropriate information from the web page. The rows of the table needed can be accessed using the find_all() function with the BeautifulSoup object using the statements below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = data.find_all('tbody')\n",
    "rows = tables[0].find_all('tr')\n",
    "\n",
    "#Here, the variable tables gets the body of all the tables in the web page and the\n",
    "#  variable rows gets all the rows of the first table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now iterate over the rows to find the required data. Use the code shown below to extract the information.\n",
    "\n",
    "for row in rows:\n",
    "    if count<50:\n",
    "        col = row.find_all('td')\n",
    "        if len(col)!=0:\n",
    "            data_dict = {\"Average Rank\": col[0].contents[0],\n",
    "                         \"Film\": col[1].contents[0],\n",
    "                         \"Year\": col[2].contents[0]}\n",
    "            df1 = pd.DataFrame(data_dict, index=[0])\n",
    "            df = pd.concat([df,df1], ignore_index=True)\n",
    "            count+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code functions as follows.\n",
    "\n",
    "1. Iterate over the contents of the variable rows.\n",
    "2. Check for the loop counter to restrict to 50 entries.\n",
    "3. Extract all the td data objects in the row and save them to col.\n",
    "4. Check if the length of col is 0, that is, if there is no data in a current row. This is important since, many timesm there are merged rows that are not apparent in the web page appearance.\n",
    "5. Create a dictionary data_dict with the keys same as the columns of the dataframe created for recording the output earlier and corresponding values from the first three headers of data.\n",
    "6. Convert the dictionary to a dataframe and concatenate it with the existing one. This way, the data keeps getting appended to the dataframe with every iteration of the loop.\n",
    "7. Increment the loop counter.\n",
    "8. Once the counter hits 50, stop iterating over rows and break the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Average Rank                                           Film  Year\n",
      "0             1                                  The Godfather  1972\n",
      "1             2                                   Citizen Kane  1941\n",
      "2             3                                     Casablanca  1942\n",
      "3             4                         The Godfather, Part II  1974\n",
      "4             5                            Singin' in the Rain  1952\n",
      "5             6                                         Psycho  1960\n",
      "6             7                                    Rear Window  1954\n",
      "7             8                                 Apocalypse Now  1979\n",
      "8             9                          2001: A Space Odyssey  1968\n",
      "9            10                                  Seven Samurai  1954\n",
      "10           11                                        Vertigo  1958\n",
      "11           12                                    Sunset Blvd  1950\n",
      "12           13                                   Modern Times  1936\n",
      "13           14                             Lawrence of Arabia  1962\n",
      "14           15                             North by Northwest  1959\n",
      "15           16                                      Star Wars  1977\n",
      "16           17                                       Parasite  2019\n",
      "17           18                               Schindler's List  1993\n",
      "18           19  Lord of the Rings: The Fellowship of the Ring  2001\n",
      "19           20                           Shawshank Redemption  1994\n",
      "20           21                          It's a Wonderful Life  1946\n",
      "21           22                                   Pulp Fiction  1994\n",
      "22           23                              Avengers: Endgame  2019\n",
      "23           24                                    City Lights  1931\n",
      "24           25                One Flew Over the Cuckoo's Nest  1975\n",
      "25           26                                     Goodfellas  1990\n",
      "26           27                        Raiders of the Lost Ark  1981\n",
      "27           28                                   12 Angry Men  1957\n",
      "28           29                       The Silence of the Lambs  1991\n",
      "29           30                                    Taxi Driver  1976\n",
      "30           31                            Saving Private Ryan  1998\n",
      "31           32                     E.T. the Extra Terrestrial  1982\n",
      "32           33                                          Alien  1979\n",
      "33           34              Spider-Man: Into the Spider-verse  2018\n",
      "34           35                                   Blade Runner  1982\n",
      "35           36                               Double Indemnity  1944\n",
      "36           37                                The Dark Knight  2008\n",
      "37           38                               The Wizard of Oz  1939\n",
      "38           39  Star Wars: Episode V- The Empire Strikes Back  1980\n",
      "39           40                                  The Searchers  1956\n",
      "40           41                             Mad Max: Fury Road  2015\n",
      "41           42                                      Inception  2010\n",
      "42           43          Lord of the Rings: Return of the King  2003\n",
      "43           44                                     The Matrix  1999\n",
      "44           45                                     Fight Club  1999\n",
      "45           46                             Back to the Future  1985\n",
      "46           47                          It Happened One Night  1934\n",
      "47           48                The Good, the Bad, and the Ugly  1966\n",
      "48           49              Lord of the Rings: The Two Towers  2002\n",
      "49           50                                  All About Eve  1950\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the data\n",
    "After the dataframe has been created, you can save it to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the required data in a database, we need to initialize a connection to the database, \n",
    "# save the dataframe as a table, and then close the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_name)\n",
    "df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the code\n",
    "\n",
    "Execute the code using **Run All**\n",
    "\n",
    "The CSV and the DB files are also created under the project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice problems\n",
    "\n",
    "Try the following practice problems to test your understanding of the lab. Please note that the solutions for the following are not shared. You are encouraged to use the discussion forums in case you need help.\n",
    "\n",
    "Modify the code to extract Film, Year, and Rotten Tomatoes' Top 100 headers.\n",
    "\n",
    "Restrict the results to only the top 25 entries.\n",
    "\n",
    "Filter the output to print only the films released in the 2000s (year 2000 included)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Performed the following:\n",
    "\n",
    "- Analyzed HTML pages for relevant data position.\n",
    "\n",
    "- Used BeautifulSoup, requests and pandas libraries for web scraping the required information.\n",
    "\n",
    "- Stored the extracted data as a CSV file and SQL Database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
