# Extract, Transform, and Load Data using Python

Introduction

Extract, Transform and Load (ETL) operations are of extreme importance in the role of a Data engineer. A data engineer extracts data from multiple sources and different file formats, transforms the extracted data to predefined settings and then loads the data to a database for further processing. In this lab, you will get hands-on practice of performing these operations.

## Objectives
- Read CSV, JSON, and XML file types.
- Extract the required data from the different file types.
- Transform data to the required format.
- Save the transformed data in a ready-to-load format, which can be loaded into an RDBMS.

## Summary: Extract, Transform, Load (ETL)

- ETL is the process of extracting large amounts of data from multiple sources and formats and transforming it into one specific format before loading it into a database or target file. 

- Web scraping is a process that can be used to extract information automatically from a website using the two Python modules “Requests” and “BeautifulSoup”.

- xml library can be used to parse XML, and the pandas library to parse CSV and JSON data. 

- to_sql() method can be used to load a Pandas data frame to an SQL database object.

- Pandas read_sql() method can be used to query a database table. This function returns a Pandas data frame with the output to the query.

- Processed data can be stored as a table in a database and retrieved from it using queries, with the Pandas and SQLite libraries.